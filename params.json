{
  "name": "Streams-flink",
  "tagline": "",
  "body": "# Streams Flink #\r\n\r\nThis project tries to slightly modify [streams-storm](https://bitbucket.org/cbockermann/streams-storm) project in order to adapt it to ``Flink``. This way we can achieve parsing of XML configuration files for ``streams framework`` and translating them into ``Flink`` topology.\r\n\r\nThe XML definition of ``streams`` process has not been changed.\r\nWe can still use ``copies`` attribute in ``<process ...>`` tag in order to controll the level of parallelism.\r\nEach copy is then mapped to a task slot inside of the Flink cluster.\r\nWe have support for ``services`` and ``queues``. \r\nEach ``process``, e.g. as the following\r\n```\r\n<process input=\"data\" id=\"extraction\" copies=\"1\">\r\n```\r\n\r\nis translated to a flatMap function with parallelism and name set over ``copies`` and ``id``:\r\n\r\n```\r\nDataStream<Data> dataStream = source\r\n\t\t.flatMap(function)\r\n        .setParallelism(Integer.parseInt(element.getAttribute(\"copies\")))\r\n        .name(element.getAttribute(\"id\"));\r\n```\r\n\r\nWhile using another process (e.g. reading from a second source or a queue) tasks may be mapped onto the same physical machine.\r\nHence, e.g. if we have two machines with 6 cores each (task slots) and have two subprocesses with the level of parallelism set to 6 each, they might both run on the same task slots on one single machine. \r\n\r\nThe easiest way to start a Flink job is to use the submit script by Flink itself:\r\n\r\n```\r\n./bin/flink run --jobmanager <jobmanager-address>:6123 -p <parallelism-level> <jar-file> <further-arguments>\r\n```",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}